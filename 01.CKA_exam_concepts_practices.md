# CKA Exam Preparation

CKA = Certified Kubernetes Administrator</br>
Understand the CKA concept, prepare for the exam and practice, practice practices!

***

# Kubernetes Administration

**Disclaimer**: This is not likely a comprehensive list as the exam will be a moving target with the fast pace of k8s development - please make a pull request if there something wrong or that should be added, or updated in here.

Ensure you have the right version of Kubernetes documentation selected (e.g. v1.16 as of 20th Nov. 2019 exam) especially for API objects and annotations. This release removes several deprecated API's.  

## Exam Objectives

These are the exam objectives you review and understand in order to pass the test.

* [CNCF Exam Curriculum repository ](https://github.com/cncf/curriculum)


## [1. Core Concepts](https://kubernetes.io/docs/concepts/) 19%
<details> <summary> Details </summary>

* [Understand the Kubernetes API primitives](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/)
  * [concepts: Kubernetes Objects](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/)
  * youtube: [Kubernetes Webinar Series - Kubernetes Architecture 101](https://www.youtube.com/watch?v=zeS6OyDoy78)
* [Understand the Kubernetes cluster architecture](https://kubernetes.io/docs/concepts/overview/components/)
  * youtube: [A Technical Overview of Kubernetes (CoreOS Fest 2015) by Brendan Burns](https://www.youtube.com/watch?v=WwBdNXt6wO4)
* [Understand Services and other network primitives](https://kubernetes.io/docs/concepts/services-networking/service/)
  * youtube: [Life of a Packet [I] - Michael Rubin, Google](https://www.youtube.com/watch?v=0Omvgd7Hg1I)
  * youtube: [The ins and outs of networking in Google Container Engine and Kubernetes (Google Cloud Next '17)](https://www.youtube.com/watch?v=y2bhV81MfKQ)

* Architecture diagram
![architecture-diagram](images/kubernetes-architecture.png)
![port](images/controlplane.jpeg)

    **Master Node**
    - kube-apiserver: front-end of the cluster that services REST operations and connects to the etcd database
    - kube-scheduler: schedules Pods on specific nodes based on labels, taints, and tolerations set for the Pods
    - etcd: a B+tree key-value store that keeps the current cluster state
    - kube-controller-manager: manages current state of the cluster 
    - cloud-controller-manager: interacts with outside cloud managers 
    - Different optionals add-ons: 
        - DNS
        - Dashboard
        - Cluster level resource monitoring 
        - Cluster level logging 

    **Worker Node**
    - kubelet: passes requests to the container engine to ensure that Pods are available 
    - kube-proxy: runs on every node and uses iptables to provide an interface to connect to Kubernetes components
    - container runtime: takes care of actually running the containers 
    - supervisord: monitors and guarantee the availability of the kubelet and docker processes
    - network agent: implements software defined network solutions, such as weave
    - logging: the CNCF Project Fluentd is used for unified logging in the cluster. A Fluentd agent must be installed on the K8s nodes. 

![master](images/master.jpeg)

*  Example Nginx deployment:
```yaml
apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```

**Controlling API Access**
- API access is regulated by RBAC (Role Based Access Control)
- In RBAC, user accounts are identified as a set of certificates associated to name, defined in ~/.kube/conf (or /etc/kubernetes/admin.conf)
  ```bash
  current-context: kubernetes-admin@kubernetes
  kind: Config
  users:
  - name: kubernetes-admin
    user:
      client-certificate-data: LS0t....
      client-key-data: LS0t...
  ```
- Kubernetes distinguishes between the concept of a user account and a service account for a number of reasons:
  - User accounts are for humans. Service accounts are for processes, which run in pods.
  - User accounts are intended to be global and unique across all namespaces of a cluster. Service accounts are namespaced.
  - Typically, a cluster's User accounts might be synced from a corporate database, where new user account creation requires special privileges and is tied to complex business processes. Service account creation is intended to be more lightweight, allowing cluster users to create service accounts for specific tasks (i.e. principle of least privilege).
- Use kubectl auth can-i to verify what you can do with current credentials. 
  - kubectl auth can-i create deployments
  - kubectl auth can-i create pods --as linda
  - kubectl auth can-i create pods --as linda --namespaces apps

**Accessing the API**
- `kubectl api-resources` show API groups and resources within the APIs
- `kubectl api-versions`
- `kubectl explain`
- Kubectl auto-completion: `kubectl completion bash >> ~/.bashrc`. Relogin with user, and you should be able to do the kubectl auto-completion
- Or, you can make it available for all users by adding the below command: 
  ```bash
  kubectl completion bash >> /etc/bash_completion.d/kubectl
  ```

**Using `curl` to access the API**
![curl](images/curl.jpeg)

- If the appropriate certificates are used, the API can be accessed by using **curl**
  - ```bash
    curl --cert myuser.pem --key myuser-key.pem --cacert /root/myca.pem https://controller:6443/api/v1
    ```
- **kubectl proxy** enable easy access to API without using certificates
  ```bash
  kubectl proxy --port 8001 &
  curl http://localhost:8001
  curl http://localhost:8001/api/v1/namespaces/default/pods
  kubectl get pods -n default -o json   # is giving the same result with the above command
  ```

**Understanding `etcdctl`**
- The `etcdctl` command can be used to interrogate and manage etcd database
- Different of versions exist: `etcdctl2` is to interract with v2 of API 
- `etcdctl` is version independent
- You need to install the package in advance: 
  ```bash
  yum install -y etcd
  ```
- Different version of etcd
  ```bash
  etcdctl --help  # This will give the default etcdctl version 2
  ETCDCTL_API=3 etcdctl --help  # This will give the etcdctl version 3
  ```


</details>


## [2. Installation, Configuration and Validation](https://github.com/kelseyhightower/kubernetes-the-hard-way/tree/f9486b081f8f54dd63a891463f0b0e783d084307) 12%
<details> <summary> Details </summary>

* Design a Kubernetes cluster
* [Install Kubernetes masters and nodes, including the use of TLS bootstrapping](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)
* [Configure secure cluster communications](https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/)
* [Configure a Highly-Available Kubernetes cluster](https://kubernetes.io/docs/admin/high-availability/)
* [Know where to get the Kubernetes release binaries](https://kubernetes.io/docs/getting-started-guides/binary_release/#prebuilt-binary-release)
* [Provision underlying infrastructure to deploy a Kubernetes cluster](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/f9486b081f8f54dd63a891463f0b0e783d084307/docs/01-infrastructure-gcp.md)
* [Choose a network solution](https://kubernetes.io/docs/concepts/cluster-administration/networking/)
* Choose your Kubernetes infrastructure configuration
* Run end-to-end tests on your cluster    
  * Some simple commands will cover most cases:
```
$ kubectl cluster-info
$ kubectl get nodes
$ kubectl get componentstatuses
$ kubectl get pods -o wide --show-labels --all-namespaces
$ kubectl get svc  -o wide --show-labels --all-namespaces
```

* Analyse end-to-end tests results.
* Install and use kubeadm to install, configure, and manage Kubernetes clusters.

* For more advanced end to end testing, which may not be covered on the exam, also see:
     * [End-To-End Testing in Kubernetes](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-testing/e2e-tests.md)
     * [Using CNCF k8s conformance](https://github.com/cncf/k8s-conformance/blob/master/instructions.md)
     * [Heptio Sonobuoy Scanner](https://scanner.heptio.com/)

**Minikube architecture**
- Minikube is a minimum environment that manage to be functioned as Master and Worker node in single machine. 
- Minikube is good for CKAD practise, but a little difficult for CKA. 
- Architecture images: 

![minikube-architecture](images/minikube-architecture.jpeg)

- **Kubernetes Cluster Installation**
    1. Cluster Node Requirement
        - To setup a Kubernetes on-premise cluster, **kubeadm** is used
        - On this Lab, we use 1 control node (master) and 2 worker node
        - Install the CentOS 7.x Minimal
        - Turn off swap space as required by kubelet to work (comment out swap on /etc/fstab and reboot the machine)
        - Disable the firewall or open appropriate port in the firewall
    2. **ON ALL NODES**
        - Lab environment: 
            - Master/Control Plane
                - 2 CPU (minimum requirement, otherwise `kubeadm init` will error)
                - 2 GB Memory
                - master.example.com
            - Worker
                - 1 GB Memory
                - worker1.example.com
                - worker2.example.com
        - Disable swap
        - Docker Runtime
            - Use docker-ce (community-edition)
        - Install base components
            ```bash
            yum install -y vim git bash-completion
            ```
        - Install some component with script
            ```bash
            git clone https://github.com/fahmifahim/kubernetes-cka.git
            cd kubernetes-cka/installation
            chmod u+x ./setup-docker.sh
            ./setup-docker.sh
            chmod u+x ./setup-kubetools.sh
            ./setup-kubetools.sh
            ```
        - Disabling firewall
        - Setting up kubernetes repo
        - Set SELinux in permisive mode
        - Install `kubelet` `kubeadm` `kubectl`
        - Setup hostname resolving on /etc/hosts

            <details><summary>setup-docker.sh and setup-kubetools.sh</summary>

            - setup-docker.sh

            ```bash
            #!/bin/bash
            # script that runs 
            # https://kubernetes.io/docs/setup/production-environment/container-runtime

            yum install -y vim yum-utils device-mapper-persistent-data lvm2
            yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

            # notice that only verified versions of Docker may be installed
            # verify the documentation to check if a more recent version is available

            yum install -y docker-ce
            [ ! -d /etc/docker ] && mkdir /etc/docker

            cat > /etc/docker/daemon.json <<EOF
            {
            "exec-opts": ["native.cgroupdriver=systemd"],
            "log-driver": "json-file",
            "log-opts": {
                "max-size": "100m"
            },
            "storage-driver": "overlay2",
            "storage-opts": [
                "overlay2.override_kernel_check=true"
            ]
            }
            EOF

            cat >> /etc/hosts << EOF
            {
                192.168.11.14	master.example.com control master       # --> change this IP as your Lab environment
                192.168.11.15	worker1.example.com worker1             # --> change this IP as your Lab environment
                192.168.11.16	worker2.example.com worker2             # --> change this IP as your Lab environment
            }
            EOF

            mkdir -p /etc/systemd/system/docker.service.d

            systemctl daemon-reload
            systemctl restart docker
            systemctl enable docker

            systemctl disable --now firewalld
            ```

            - setup-kubetools.sh
            ```bash
            #!/bin/bash
            # kubeadm installation instructions as on
            # https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

            cat <<EOF > /etc/yum.repos.d/kubernetes.repo
            [kubernetes]
            name=Kubernetes
            baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
            enabled=1
            gpgcheck=1
            repo_gpgcheck=1
            gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
            EOF

            # Set SELinux in permissive mode (effectively disabling it)
            setenforce 0
            sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

            # disable swap (assuming that the name is /dev/centos/swap
            sed -i 's/^\/dev\/mapper\/centos-swap/#\/dev\/mapper\/centos-swap/' /etc/fstab
            swapoff /dev/mapper/centos-swap

            yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

            systemctl enable --now kubelet

            # Set iptables bridging
            cat <<EOF >  /etc/sysctl.d/k8s.conf
            net.bridge.bridge-nf-call-ip6tables = 1
            net.bridge.bridge-nf-call-iptables = 1
            EOF
            sysctl --system
            ```

            </details>

    3. **ON MASTER/CONTROL PLANE**
        - `kubeadm init`

            <details><summary> kubeadm init (logs) </summary>
            
            ```bash
            kubeadm init
            W0506 16:48:54.526194    9665 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
            [init] Using Kubernetes version: v1.18.2
            [preflight] Running pre-flight checks
            [preflight] Pulling images required for setting up a Kubernetes cluster
            [preflight] This might take a minute or two, depending on the speed of your internet connection
            [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
            [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
            [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
            [kubelet-start] Starting the kubelet
            [certs] Using certificateDir folder "/etc/kubernetes/pki"
            [certs] Generating "ca" certificate and key
            [certs] Generating "apiserver" certificate and key
            [certs] apiserver serving cert is signed for DNS names [master.example.com kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.11.14]
            [certs] Generating "apiserver-kubelet-client" certificate and key
            [certs] Generating "front-proxy-ca" certificate and key
            [certs] Generating "front-proxy-client" certificate and key
            [certs] Generating "etcd/ca" certificate and key
            [certs] Generating "etcd/server" certificate and key
            [certs] etcd/server serving cert is signed for DNS names [master.example.com localhost] and IPs [192.168.11.14 127.0.0.1 ::1]
            [certs] Generating "etcd/peer" certificate and key
            [certs] etcd/peer serving cert is signed for DNS names [master.example.com localhost] and IPs [192.168.11.14 127.0.0.1 ::1]
            [certs] Generating "etcd/healthcheck-client" certificate and key
            [certs] Generating "apiserver-etcd-client" certificate and key
            [certs] Generating "sa" key and public key
            [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
            [kubeconfig] Writing "admin.conf" kubeconfig file
            [kubeconfig] Writing "kubelet.conf" kubeconfig file
            [kubeconfig] Writing "controller-manager.conf" kubeconfig file
            [kubeconfig] Writing "scheduler.conf" kubeconfig file
            [control-plane] Using manifest folder "/etc/kubernetes/manifests"
            [control-plane] Creating static Pod manifest for "kube-apiserver"
            [control-plane] Creating static Pod manifest for "kube-controller-manager"
            W0506 16:50:37.332345    9665 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
            [control-plane] Creating static Pod manifest for "kube-scheduler"
            W0506 16:50:37.334387    9665 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
            [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
            [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
            [apiclient] All control plane components are healthy after 35.529212 seconds
            [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
            [kubelet] Creating a ConfigMap "kubelet-config-1.18" in namespace kube-system with the configuration for the kubelets in the cluster
            [upload-certs] Skipping phase. Please see --upload-certs
            [mark-control-plane] Marking the node master.example.com as control-plane by adding the label "node-role.kubernetes.io/master=''"
            [mark-control-plane] Marking the node master.example.com as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
            [bootstrap-token] Using token: 121svj.8urd4tdpt7r51p5n
            [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
            [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
            [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
            [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
            [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
            [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
            [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
            [addons] Applied essential addon: CoreDNS
            [addons] Applied essential addon: kube-proxy

            Your Kubernetes control-plane has initialized successfully!

            To start using your cluster, you need to run the following as a regular user:

            mkdir -p $HOME/.kube
            sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
            sudo chown $(id -u):$(id -g) $HOME/.kube/config

            You should now deploy a pod network to the cluster.
            Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
            https://kubernetes.io/docs/concepts/cluster-administration/addons/

            Then you can join any number of worker nodes by running the following on each as root:

            kubeadm join 192.168.11.14:6443 --token 121svj.8urd4tdpt7r51p5n \
                --discovery-token-ca-cert-hash sha256:996d622a652b5ae512c1df35bf1560c252f0f70af8efee1a893849e5a7155231

            ```
            
            </details>

        - **Take a note of the `kubeadm join`** for the Token created

    4. **STARTING THE CLUSTER**
        - Create the client configuration as a regular user account
            ```bash
            su student
            mkdir -p $HOME/.kube
            sudo cp -l /etc/kubernetes/admin.conf $HOME/.kube/config
            sudo chown $(id -u):$(id -g) $HOME/.kube/config

            # Check the config file
            ls -l $HOME/.kube/config
               -rw-------. 1 student student 5453 May  6 17:00 config
            cat $HOME/.kube/config

            # (Optional) Your root user may also copy the config file to its directory
            sudo su -
            mkdir -p $HOME/.kube
            cp -l /etc/kubernetes/admin.conf $HOME/.kube/config
                --> Now your root user can call the kubectl command too
            ```
        - Verify by `kubectl cluster-info`
            ```bash
            $ kubectl cluster-info
            Kubernetes master is running at https://192.168.11.14:6443
            KubeDNS is running at https://192.168.11.14:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

            To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
            ```
        - `kubectl get nodes` (will give you not ready status at this point. it is normal)
            ```bash
            $ kubectl get nodes
            NAME                 STATUS     ROLES    AGE   VERSION
            master.example.com   NotReady   master   16m   v1.18.2
            ```
    5. **[Installing a Pod Network Add-on](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network)**
        - A Network Add-on must be installed for pods to communicate
        - CNI (Container Network Interface) which work with add-ons to implement networking
        - Look for an add-on that supports `network-policy` as well as RBAC
        - Common pod network plugin: 
            - Weave: a common ad-on for a CNI-enabled Kubernetes cluster
            - Flannel: a layer 3 IPV4 network between cluster nodes that can use several backend mechanism such as VXLAN
            - Calico: a layer 3 network solution that uses IP encapsulation and is used in Kubernetes, OpenStack, OpenShift, Docker, etc
            - AWS VPC: network plugin commonly used for AWS environment
        - Network plugin is installed from control plane (master node)
        - In our lab, we apply the Weave plugin: 
            ```bash
            # Install the network plugin from Master node
            $ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
                serviceaccount/weave-net created
                clusterrole.rbac.authorization.k8s.io/weave-net created
                clusterrolebinding.rbac.authorization.k8s.io/weave-net created
                role.rbac.authorization.k8s.io/weave-net created
                rolebinding.rbac.authorization.k8s.io/weave-net created
                daemonset.apps/weave-net created    

            $ kubectl get pods --all-namespaces
                NAMESPACE     NAME                                         READY   STATUS    RESTARTS   AGE
                kube-system   coredns-66bff467f8-2vqrg                     1/1     Running   0          31m
                kube-system   coredns-66bff467f8-fnq7h                     1/1     Running   0          31m
                kube-system   etcd-master.example.com                      1/1     Running   0          31m
                kube-system   kube-apiserver-master.example.com            1/1     Running   0          31m
                kube-system   kube-controller-manager-master.example.com   1/1     Running   0          31m
                kube-system   kube-proxy-tbdzc                             1/1     Running   0          31m
                kube-system   kube-scheduler-master.example.com            1/1     Running   0          31m
                kube-system   weave-net-tp5b9                              2/2     Running   0          72s

            ! Make sure all the above pods are "Running" before you move forward to join the worker node to the cluster

            $ kubectl get nodes
                NAME                 STATUS   ROLES    AGE   VERSION
                master.example.com   Ready    master   33m   v1.18.2

            $ exit 

            ```
    6. **JOINING WORKER NODE**
        - Use the command from `kubeadm join` as we noted on the previous procedure
            ```bash
            kubeadm join 192.168.11.14:6443 --token 121svj.8urd4tdpt7r51p5n \
                --discovery-token-ca-cert-hash sha256:996d622a652b5ae512c1df35bf1560c252f0f70af8efee1a893849e5a7155231
            ```
        - Execute the `kubeadm join ...` at each worker nodes
            ```bash
            ssh worker1
                root@worker1's password:
                Last login: Wed May  6 17:30:47 2020 from master.example.com
            
            # Copy paste the kubeadm join
            kubeadm join 192.168.11.14:6443 --token 121svj.8urd4tdpt7r51p5n \
                --discovery-token-ca-cert-hash sha256:996d622a652b5ae512c1df35bf1560c252f0f70af8efee1a893849e5a7155231

            # Result: 
                W0506 17:32:39.026771    5170 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
                [preflight] Running pre-flight checks
                [preflight] Reading configuration from the cluster...
                [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
                [kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.18" ConfigMap in the kube-system namespace
                [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
                [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
                [kubelet-start] Starting the kubelet
                [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

                This node has joined the cluster:
                * Certificate signing request was sent to apiserver and a response was received.
                * The Kubelet was informed of the new secure connection details.

                Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
            ```

            ```bash
            [student@master root]$ kubectl get nodes
                NAME                  STATUS   ROLES    AGE     VERSION
                master.example.com    Ready    master   51m     v1.18.2
                worker1.example.com   Ready    <none>   9m37s   v1.18.2
                worker2.example.com   Ready    <none>   77s     v1.18.2
            ```
    7. Check the `kubectl config view`
        ```bash
        $ kubectl config view
            apiVersion: v1
            clusters:
            - cluster:
                certificate-authority-data: DATA+OMITTED
                server: https://192.168.11.14:6443
            name: kubernetes
            contexts:
            - context:
                cluster: kubernetes
                user: kubernetes-admin
            name: kubernetes-admin@kubernetes
            current-context: kubernetes-admin@kubernetes
            kind: Config
            preferences: {}
            users:
            - name: kubernetes-admin
            user:
                client-certificate-data: REDACTED
                client-key-data: REDACTED    
        ```

</details>

## 3. Security 12%
< details > <summary> Details </summary>

* [Securing a kubernetes cluster](https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/)
    * youtube: [Building for Trust: How to Secure Your Kubernetes Cluster [I] - Alexander Mohr & Jess Frazelle](https://www.youtube.com/watch?v=YRR-kZub0cA)
* [Know how to configure authentication and authorization](https://kubernetes.io/docs/admin/authorization/rbac/)
  * [Access the api](https://kubernetes.io/docs/admin/accessing-the-api/)
  * [Authentication](https://kubernetes.io/docs/reference/access-authn-authz/authentication/)
  * [Authorization with RBAC](https://kubernetes.io/docs/admin/authorization/rbac/)
  * [Admission Control](https://kubernetes.io/docs/admin/admission-controllers/)
* [Understand Kubernetes security primitives]
  * [Pod Security Policy](https://kubernetes.io/docs/concepts/policy/pod-security-policy/)
    * [PSP and RBAC](https://github.com/kubernetes/examples/blob/master/staging/podsecuritypolicy/rbac/README.md)
* [Know to configure network policies](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/)
  * [Blog: Kubernetes network policy](https://ahmet.im/blog/kubernetes-network-policy/)
  * [Katacoda Calico](https://www.katacoda.com/projectcalico/scenarios/calico)
* [Create and manage TLS certificates for cluster components](https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/)
* Work with images securely
* [Define security contexts](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)
* [Secure persistent key value store](https://kubernetes.io/docs/concepts/configuration/secret/)

- Understanding Certificates and API Access
  - The API serves on port 6443 by default
  - The API often is self-signed. The root certificate for the API server certificate is $USER/.kube/config
  - The user need to present its own client certificate to get access to the API

- Understanding API Access: **Authentication**
  - Authentication: **Authentication Modules** are used to include client certificates, password or tokens to handle authentication of human and service accounts
    - Human accounts typically use client certificates
    - Service accounts typically use tokens
  - Requests that can't be authenticated are rejected with HTTP status code 401
  - Remember, Kubernetes doesn't have User Objects. A User is just a set of client certificates
  - Three essential certificate components in $USER/.kube/config:
    - client-certificate-data: contains the client public key certificate
    - client-key-data: contains the client private key
    - certificate-authority-data: contains the CA public key certificate
  - Here sample of manual access of CURL: 
    ```bash
    # Define the certificate components:
    export client=$(grep client-certificate-data ~/.kube/config | cut -d " " -f 6)
    export key=$(grep client-key-data ~/.kube/config | cut -d " " -f 6)
    export auth=$(grep certificate-authority-data ~/.kube/config | cut -d " " -f 6)

    # Generate certificate files (.pem files)
    echo $client | base64 -d - > client.pem
    echo $key | base64 -d - > key.pem
    echo $auth | base64 -d - > ca.pem

    # Find out the target API server
    kubectl config view | grep server
        server: https://192.168.11.25:6443

    # Get the Pods by CURL command using the above certificate files
    curl --cert ./client.pem --key ./key.pem --cacert ./ca.pem https://192.168.11.25:6443/api/v1/pods

    ```

- Understanding API Access: **Authorization**
  - After establishing that a request comes from specific user, it must be authorized
  - Kubernetes is doing this by using `Authorization Modes`
    - ABAC (Attribute Based Access Control)
    - RBAC (Role Based Access Control)
      - Role: used to grant access to resources within a single namespace
      - ClusterRole: used to grant access to resources at a Cluster Level (beyond namespace)
      - RoleBinding: used to grant permissions defined in a role to one or more users. 
      - ClusterRoleBinding: used to grant permission at the cluster level and in all namespaces
    - Webhook (Uses HTTP POST when something happens)
  - Authorization module is specified when kube-apiserver is started
  - See the /etc/kubernetes/manifests/
    - etcd.yaml
    - kube-apiserver.yaml   --> this file to change the Authorization
    - kube-controller-manager.yaml
    - kube-scheduler
  - Here is inside the `kube-apiserver` pods
    ```yaml
    kind: Pod
    metadata:
      name: kube-apiserver
      namespace: kube-system
    spec:
      containers:
      - command:
        - kube-apiserver
        - --authorization-mode=Node,RBAC
        - --client-ca-file=/etc/kubernetes/pki/ca.crt
        - --enable-admission-plugins=NodeRestriction
        - --enable-bootstrap-token-auth=true
        - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
        - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
        - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
        - --etcd-servers=https://127.0.0.1:2379
        - --insecure-port=0
        - --secure-port=6443
        - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
        - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
        - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
        - --requestheader-allowed-names=front-proxy-client
    ```
- Understanding API Access: **Admission**
  - After being authorized, the admission controller is involved
  - Admission controllers implement specific functionality. Such as, ResourceQuota will ensure an object doesn't violate any existing quota rules
  - A list of additional admission can be included and activated from the /etc/kubernetes/manifests/kube-apiserver.yaml


### Managing Kubernetes User Account
  - Kubernetes has no User Objects. User accounts consist of an authorized certificate that is completed with some authorization as defined in RBAC
  - To create a user account, perform the following steps: 
    - Create a public/private key pair
    - Create a Certificate Signing Request (CSR)
    - Sign the Certificate
    - Create a configuration file that uses these keys to access the K8s cluster
    - Create an RBAC Role
    - Create an RBAC RoleBinding

#### Creating User Accounts
##### Step 1: Create a user working environment
- Create several different namespaces: 
  - kubectl create namespace staff
  - kubectl create namespace students
  - kubectl config get-contexts

<details>

```bash
kubectl get ns
    NAME              STATUS   AGE
    default           Active   20d
    dev               Active   12d
    kube-node-lease   Active   20d
    kube-public       Active   20d
    kube-system       Active   20d
    staff             Active   30s
    students          Active   23s

kubectl config get-contexts
    CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
    *         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   dev
```

</details>

##### Step 2: Create the User account (username: fahmi)
- useradd -G wheel fahmi
- passwd fahmi
- openssl genrsa -out fahmi.key 2048
- openssl req -new -key fahmi.key -out fahmi.csr -subj "/CN=fahmi/O=staff"
- openssl x509 -req -in fahmi.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out fahmi.crt -days 365

<details>

```bash
useradd -G wheel fahmi

id fahmi
    uid=1001(fahmi) gid=1001(fahmi) groups=1001(fahmi),10(wheel)

passwd fahmi
    Changing password for user fahmi.
    New password:
    BAD PASSWORD: The password fails the dictionary check - it is based on a dictionary word
    Retype new password:
    passwd: all authentication tokens updated successfully.

openssl genrsa -out fahmi.key 2048
    Generating RSA private key, 2048 bit long modulus
    ..+++
    ..................................................+++
    e is 65537 (0x10001)

ls -lth
    total 24K
    -rw-r--r--. 1 root root 1.7K May 28 21:40 fahmi.key

openssl req -new -key fahmi.key -out fahmi.csr -subj "/CN=fahmi/O=staff"

ls -lth
    total 28K
    -rw-r--r--. 1 root root  907 May 28 21:41 fahmi.csr
    -rw-r--r--. 1 root root 1.7K May 28 21:40 fahmi.key

openssl x509 -req -in fahmi.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out fahmi.crt -days 365
    Signature ok
    subject=/CN=fahmi/O=staff
    Getting CA Private Key

ls -lth
total 32K
-rw-r--r--. 1 root root  993 May 28 21:43 fahmi.crt
-rw-r--r--. 1 root root  907 May 28 21:41 fahmi.csr
-rw-r--r--. 1 root root 1.7K May 28 21:40 fahmi.key

openssl x509 -text -in fahmi.crt
    Certificate:
        Data:
            Version: 1 (0x0)
            Serial Number:
                98:c0:d3:38:5e:30:23:ff
        Signature Algorithm: sha256WithRSAEncryption
            Issuer: CN=kubernetes
            Validity
                Not Before: May 28 12:43:00 2020 GMT
                Not After : May 28 12:43:00 2021 GMT
            Subject: CN=fahmi, O=staff
            Subject Public Key Info:
                Public Key Algorithm: rsaEncryption
                    Public-Key: (2048 bit)
                    Modulus:
                        00:d1:74:a5:f4:7e:63:3c:2c:e0:f5:5c:c6:40:e1:
                        bb:61:d8:bf:c6:e1:de:05:ab:14:00:dc:7f:00:92:
                        32:dd:d7:33:7d:22:2d:aa:bd:df:e3:e2:c5:6d:f2:
                        99:1b:d4:44:b1:91:77:79:54:3e:77:49:ed:9d:aa:
                        75:90:89:80:43:5b:f5:3d:54:80:1a:81:48:e0:e8:
                        03:8b:69:c9:d7:64:3a:d2:94:6d:b5:15:40:82:55:
                        ac:b0:25:3f:6a:96:a0:b9:2d:8d:c3:f1:12:be:c6:
                        19:34:d9:16:75:b2:39:ac:24:fb:69:d1:da:4e:88:
                        db:f0:9d:cd:c0:29:99:56:fb:3e:fe:92:1b:92:34:
                        a2:4e:4f:04:76:59:b1:33:aa:0b:56:41:f0:95:21:
                        f3:8e:f3:05:44:a9:f7:c4:5c:3b:64:f1:62:12:45:
                        3a:af:e9:8b:c8:61:21:c1:c0:79:02:47:ef:ec:05:
                        1d:7f:98:1f:19:58:b2:9f:68:b2:3a:12:9c:1a:0d:
                        3e:10:e3:fc:55:81:33:4c:39:21:70:24:87:44:01:
                        aa:a9:e1:66:5c:85:14:63:86:59:65:c0:61:40:5d:
                        e6:d6:3a:c8:46:c8:51:2c:d0:e1:40:42:72:86:42:
                        3f:30:5a:7d:06:00:ed:c8:8d:c8:3c:53:b1:f9:e1:
                        a8:31
                    Exponent: 65537 (0x10001)
        Signature Algorithm: sha256WithRSAEncryption
            8c:d5:bb:d3:d1:4f:fb:73:a4:7f:59:2a:0d:0b:56:37:62:f0:
            c4:9c:91:73:2f:e2:f4:08:f9:34:d5:a8:4b:31:9b:d2:eb:15:
            a4:89:d2:91:c9:83:78:5c:98:b2:94:89:9d:84:5d:9a:c0:80:
            ea:0f:6a:e6:61:12:97:95:e9:4a:14:02:3a:b0:ab:4a:03:5b:
            f4:7f:43:f4:25:d1:86:cb:8d:a0:97:d9:06:5b:09:b1:ae:7c:
            92:f7:4c:ee:ff:a1:35:e6:63:10:f8:61:d2:b2:23:7c:a4:ce:
            0b:99:13:b5:51:86:46:14:08:5b:3e:03:69:f7:c4:9b:a7:45:
            4d:30:1e:9d:eb:af:84:2a:c3:61:01:a2:eb:1d:f9:3d:9f:35:
            a3:7c:b9:8f:72:9a:1a:a3:aa:d0:2b:86:15:f8:f3:7a:64:59:
            23:e0:d7:d7:a1:b6:b1:99:f0:77:f9:2f:e8:55:43:1e:45:1d:
            4f:6d:be:01:91:78:f7:ed:89:9e:c1:dc:ad:79:35:5a:b9:0d:
            12:31:ae:4d:ad:d8:89:5b:82:69:0f:59:3f:79:97:16:14:de:
            ef:8e:02:f2:bc:ce:b9:99:f7:0b:28:08:83:dc:02:60:3e:0f:
            23:65:f8:48:63:85:53:5e:c9:5e:47:c6:9e:8b:48:00:8e:d4:
            de:f9:71:0a
    -----BEGIN CERTIFICATE-----
    MIICsTCCAZkCCQCYwNM4XjAj/zANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwpr
    dWJlcm5ldGVzMB4XDTIwMDUyODEyNDMwMFoXDTIxMDUyODEyNDMwMFowIDEOMAwG
    A1UEAwwFZmFobWkxDjAMBgNVBAoMBXN0YWZmMIIBIjANBgkqhkiG9w0BAQEFAAOC
    AQ8AMIIBCgKCAQEA0XSl9H5jPCzg9VzGQOG7Ydi/xuHeBasUANx/AJIy3dczfSIt
    qr3f4+LFbfKZG9REsZF3eVQ+d0ntnap1kImAQ1v1PVSAGoFI4OgDi2nJ12Q60pRt
    tRVAglWssCU/apaguS2Nw/ESvsYZNNkWdbI5rCT7adHaTojb8J3NwCmZVvs+/pIb
    kjSiTk8EdlmxM6oLVkHwlSHzjvMFRKn3xFw7ZPFiEkU6r+mLyGEhwcB5Akfv7AUd
    f5gfGViyn2iyOhKcGg0+EOP8VYEzTDkhcCSHRAGqqeFmXIUUY4ZZZcBhQF3m1jrI
    RshRLNDhQEJyhkI/MFp9BgDtyI3IPFOx+eGoMQIDAQABMA0GCSqGSIb3DQEBCwUA
    A4IBAQCM1bvT0U/7c6R/WSoNC1Y3YvDEnJFzL+L0CPk01ahLMZvS6xWkidKRyYN4
    XJiylImdhF2awIDqD2rmYRKXlelKFAI6sKtKA1v0f0P0JdGGy42gl9kGWwmxrnyS
    90zu/6E15mMQ+GHSsiN8pM4LmRO1UYZGFAhbPgNp98Sbp0VNMB6d66+EKsNhAaLr
    Hfk9nzWjfLmPcpoao6rQK4YV+PN6ZFkj4NfXobaxmfB3+S/oVUMeRR1Pbb4BkXj3
    7YmewdyteTVauQ0SMa5NrdiJW4JpD1k/eZcWFN7vjgLyvM65mfcLKAiD3AJgPg8j
    ZfhIY4VTXsleR8aei0gAjtTe+XEK
    -----END CERTIFICATE-----
```

</details>

</details>

## [4. Networking](https://kubernetes.io/docs/concepts/cluster-administration/networking/) 11%
<details> <summary> Details </summary>

* [Understand the networking configuration on the cluster nodes](https://kubernetes.io/docs/concepts/cluster-administration/networking/)
* Understand Pod networking concepts
  * youtube: [The ins and outs of networking in Google Container Engine and Kubernetes (Google Cloud Next '17)](https://www.youtube.com/watch?v=y2bhV81MfKQ)
  * youtube: [Networking with Kubernetes](https://www.youtube.com/watch?v=WwQ62OyCNz4)
  * [Illustrated Guide To Kubernetes Networking by Tim Hockin](https://speakerdeck.com/thockin/illustrated-guide-to-kubernetes-networking)
* Understand service networking
  * youtube: [Life of a Packet [I] - Michael Rubin, Google](https://www.youtube.com/watch?v=0Omvgd7Hg1I)
* [Deploy and configure network load balancer](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/)
* [Know how to use Ingress rules](https://kubernetes.io/docs/concepts/services-networking/ingress/)
* [Know how to configure and use the cluster DNS](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)
* [Understand CNI](https://github.com/containernetworking/cni)
  * [More information on CNI](http://www.dasblinkenlichten.com/understanding-cni-container-networking-interface/)


- Pods Network diagram
![network](images/network.jpeg)
  - Pod is holding an IP address. 
  - In a pod there is an administrative container called **Pause Container**. It responsible for managing the IP address
  - In multiple-containers scenario, each container need to find a way to communicate each other. 
    - IPC : Inter Process Communication
    - Shared Volume
    - Loopback address
  - Pause Container can be determined by the below command on Master node:
    ```bash
    --> In this example we are using net-demo Pod
    $ kubectl exec -it net-demo -c container1-busybox -- ip  a show
        1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
            link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
            inet 127.0.0.1/8 scope host lo
              valid_lft forever preferred_lft forever
        172: eth0@if173: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1376 qdisc noqueue
            link/ether 02:c8:4b:fa:b2:55 brd ff:ff:ff:ff:ff:ff
            inet 10.44.0.6/12 brd 10.47.255.255 scope global eth0
              valid_lft forever preferred_lft forever

    $ kubectl exec -it net-demo -c container2-busybox -- ip  a show
        1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
            link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
            inet 127.0.0.1/8 scope host lo
              valid_lft forever preferred_lft forever
        172: eth0@if173: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1376 qdisc noqueue
            link/ether 02:c8:4b:fa:b2:55 brd ff:ff:ff:ff:ff:ff
            inet 10.44.0.6/12 brd 10.47.255.255 scope global eth0
              valid_lft forever preferred_lft forever

    $ kubectl get pod -o wide
        NAME             READY   STATUS    RESTARTS   AGE     IP          NODE                     NOMINATED NODE   READINESS GATES
        net-demo         2/2     Running   0          7m29s   10.44.0.6   vb-worker1.example.com   <none>           <none>

    --> Notice that the IP on containers are exactly the same with IP on Pod
    --> Observe deeply on the Worker node and reveal the docker process, and find the `pause` container. 
    --> You will find the `pause` container for net-demo Pod

    $ docker ps | grep pause | grep net-demo
        a7ef313032af        k8s.gcr.io/pause:3.2   "/pause"                 12 minutes ago      Up 11 minutes                           k8s_POD_net-demo_dev_a9582170-0672-49eb-b2b7-322fcbf39d66_0

    ```
- **Service**
  - Pod IP addresses are volatile, so something else is needed: The Service. 
  - Service provide access to Pod endpoints by using **Labels**
  - Service load-balance workload between the Pods that are accessible as an endpoint 
  - Service Types: 
    - ClusterIP: the service is internally exposed and is reachable only within the cluster
    - NodePort: the service is exposed at each node's IP address at a static IP address. The service can be reached from outside the cluster at `nodeip:nodeport`
    - LoadBalancer: the cloud provider offers a load balancer that routes traffic to either NodePort or ClusterIP based services
    - ExternalName: the service is mapped to an external name that is implemented as a DNS CNAME record
  - **Ingress**: 
    - The purpose of Ingress is to represent a URL, or something that is easy to use for Users. 
    - Ingress is bringing users to services. 
![service](images/service2.jpeg)

</details>

## 5. Cluster Maintenance 11%
<details> <summary> Details </summary>

* [Understand Kubernetes cluster upgrade process](https://kubernetes.io/docs/getting-started-guides/ubuntu/upgrades/)
    * Best resource upgrade is to watch [TGI Kubernetes 011: Upgrading to 1.8 with kubeadm](https://youtu.be/x9doB5eJWgQ)
* [Facilitate operating system upgrades](https://cloud.google.com/container-engine/docs/clusters/upgrade) #need review to make it more platform agnostic
* [Implement backup and restore methodologies](https://kubernetes.io/docs/getting-started-guides/ubuntu/backups/)
* Other: 
    * [Etcd management/backups/restore](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd)

</details>

## [6. Troubleshooting](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/) 10%
<details> <summary> Details </summary>

* [Troubleshoot application failure](https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/)
  * [Application Introspection and Debugging](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/)
  * [Services](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/)
* [Troubleshoot control plane failure](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/)
  * youtube [Kubernetes Day 2: Cluster Operations [I] - Brandon Philips, CoreOS](https://www.youtube.com/watch?v=U1zR0eDQRYQ)
  * Safaribooksonline: [https://www.safaribooksonline.com/library/view/oscon-2016-video/9781491965153/video246982.html](https://www.safaribooksonline.com/library/view/oscon-2016-video/9781491965153/video246982.html)
* [Troubleshoot worker node failure](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/)
* Troubleshoot networking

</details>

## [7. Storage](https://kubernetes.io/docs/concepts/storage/volumes/) 7%
<details> <summary> Details </summary>

![storageType](images/storagetype.jpeg)

* [Understand persistent volumes and know how to create them](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
* [Understand access modes for volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes)
* [Understand persistent volume claims primitive](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)
* [Understand Kubernetes storage objects](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes)
* [Know how to configure applications with persistent storage](https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/)

- PV and PVC diagram
![pv-pvc](images/pv-pvc.jpeg)
  - Pods only care about PVC. Pod doesn't care about PV. 
  - Multiple pods can bind to a single PVC. So that multiple pods can write data to the same PV. 
  - PVC and PV bound by the same `accessMode`. PVC search the same `accessMode` on PV. 
  - PV can't be mounted by multiple PVCs. PV can be bounded by only one PVC. 1 vs 1. 
  - PVC bounded PV randomly (? need further labs check)
  - PVC will be `pending` if it doesn't find correlate PV. 

- Decoupling concept
![decoupling-cm-secret](images/decoupling-cm-secret.jpeg)
  - Secret is base64 encoded `ConfigMap`. But notice that it is not encrypted. If you store sensitive credential information, don't forget to encrypt the secret and mount to your pods. 

</details>

## 8. Application Lifecycle Management 8%
<details> <summary> Details </summary>

* [Understand Deployments and how to perform rolling updates and rollbacks](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
* [Know various ways to configure applications](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/)
* [Know how to scale applications](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#scaling-your-application)
* Understand the primitives necessary to create a self-healing application

**Undestanding Namespaces**
- Namespace are a Linux kernel feature that is leveraged up to Kubernetes level
- Namespace implement strict resource separation
- Use namespaces to separate different customer environments within one Kubernetes cluster
- Four namespaces which are defined when a cluster is created: 
  - default: this is where all Kubernetes resources are created by default
  - kube-node-lease: an administrative namespace where node lease information is stored
  - kube-public: a namespace is world-readable
  - kube-system: contains all infrastructure pods
```bash
$ kubectl get ns 
    NAME              STATUS   AGE
    default           Active   8d
    kube-node-lease   Active   8d
    kube-public       Active   8d
    kube-system       Active   8d

$ kubectl get all --all-namespaces
    NAMESPACE     NAME                                                READY   STATUS    RESTARTS   AGE
    kube-system   pod/coredns-66bff467f8-5jzx2                        1/1     Running   4          8d
    kube-system   pod/coredns-66bff467f8-v6lct                        1/1     Running   4          8d
    kube-system   pod/etcd-vb-master.example.com                      1/1     Running   7          8d
    kube-system   pod/kube-apiserver-vb-master.example.com            1/1     Running   14         8d
    kube-system   pod/kube-controller-manager-vb-master.example.com   1/1     Running   12         8d
    kube-system   pod/kube-proxy-bvqs8                                1/1     Running   5          8d
    kube-system   pod/kube-proxy-hxpjt                                1/1     Running   3          8d
    kube-system   pod/kube-scheduler-vb-master.example.com            1/1     Running   12         8d
    kube-system   pod/weave-net-cc498                                 2/2     Running   18         8d
    kube-system   pod/weave-net-x86h6                                 2/2     Running   10         8d

    NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
    default       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  8d
    kube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   8d

    NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
    kube-system   daemonset.apps/kube-proxy   2         2         2       2            2           kubernetes.io/os=linux   8d
    kube-system   daemonset.apps/weave-net    2         2         2       2            2           <none>                   8d

    NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
    kube-system   deployment.apps/coredns   2/2     2            2           8d

    NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE
    kube-system   replicaset.apps/coredns-66bff467f8   2         2         2       8d

$ kubectl create ns dev
    namespace/dev created

$ kubectl describe ns dev
    Name:         dev
    Labels:       <none>
    Annotations:  <none>
    Status:       Active

    No resource quota.

    No LimitRange resource.

$ kubectl get ns dev
    NAME   STATUS   AGE
    dev    Active   36s

$ kubectl get ns dev -o yaml
    apiVersion: v1
    kind: Namespace
    metadata:
      creationTimestamp: "2020-05-16T12:24:35Z"
      managedFields:
      - apiVersion: v1
        fieldsType: FieldsV1
        fieldsV1:
          f:status:
            f:phase: {}
        manager: kubectl
        operation: Update
        time: "2020-05-16T12:24:35Z"
      name: dev
      resourceVersion: "938534"
      selfLink: /api/v1/namespaces/dev
      uid: 5f859fb3-7ec8-4310-af2c-cd08e40921ce
    spec:
      finalizers:
      - kubernetes
    status:
      phase: Active

```

**Create Deployment**
```bash
$ kubectl create deployment newnginx --image=nginx -n dev 

$ kubectl get deployment -n dev --show-labels

$ kubectl get all
    NAME                            READY   STATUS    RESTARTS   AGE
    pod/newnginx-5f6bd64bcc-95gp5   1/1     Running   0          36s

    NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
    deployment.apps/newnginx   1/1     1            1           36s

    NAME                                  DESIRED   CURRENT   READY   AGE
    replicaset.apps/newnginx-5f6bd64bcc   1         1         1       36s
```

**Scale Deployment**
```bash
$ kubectl scale deployment newnginx -n dev --replicas=3
$ kubectl edit deployment newnnginx -n dev
```

**Deploy App Without Downtime**
- Strategy to upgrade App (eg:change App Image) using the `maxSurge` and `maxUnavailable`, to make sure old app and new app are deployed accordingly without affecting the down of your application. 
- **maxSurge**
  - `maxSurge` specifies the maximum number of Pods that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example, 10%). 
  - For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the total number of Pods running at any time during the update is at most 130% of desired Pods.
- **maxUnavailable**
  - `maxUnavailable` specifies the maximum number of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example, 10%). 
  - For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available at all times during the update is at least 70% of the desired Pods.

![maxSurge_maxUnavailable](images/maxSurgeUnavailable.jpeg)

#### Rolling Update Deployment 
- `kubectl rollout history deployment`
  ```bash
  $ kubectl rollout history deployment rolling-nginx
      deployment.apps/rolling-nginx
      REVISION  CHANGE-CAUSE
      1         <none>
      2         <none>
      3         <none>
  ```
- `kubectl rollout status deployment <deployment-name>`
  ```bash
  $ kubectl rollout status deployment rolling-nginx
      deployment "rolling-nginx" successfully rolled out
  ```
- `kubectl rollout history deployment rolling-nginx --revision 9`
  ```bash
  $ kubectl rollout history deployment rolling-nginx --revision 9
      deployment.apps/rolling-nginx with revision #9
      Pod Template:
        Labels:	app=nginx
        pod-template-hash=567df7d6d9
        Containers:
        nginx:
          Image:	nginx:1.16
          Port:	<none>
          Host Port:	<none>
          Environment:	<none>
          Mounts:	<none>
        Volumes:	<none>
  ```  
- `kubectl rollout undo deployment <deployment-name>`

</details>

## 9. Scheduling 5%
<details> <summary> Details </summary>

* [Use label selectors to schedule Pods](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
* [Understand the role of DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)
* [Understand how resource limits can affect Pod scheduling](https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/)
* [Understand how to run multiple schedulers and how to configure Pods to use them](https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/)
* [Manually schedule a pod without a scheduler](https://kubernetes.io/docs/tasks/administer-cluster/static-pod/)
   If you require a pod to start on a specific node, you can specify this in POD spec.nodeName, that is what DaemonSets do.
* [Display scheduler events](https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work/28874577#28874577)
  /var/log/kube-scheduler.log on the control/master node
  or use `kubectl describe` as in
```
  $kubectl describe pods <POD NAME UNDER Investigation>  | grep -A7 ^Events
```
* [Know how to configure the Kubernetes scheduler](https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/)

- **Scheduler Policy**
  - **kube-scheduler** works with Filtering and Scoring to determine where to run the Pod
    - In `Filtering`, a set of nodes is found where it is feasible to schedule the Pod because the node meets the required resources. 
      - PodFitsHostPorts: if free network ports are available
      - PodFitsResources: if the node has sufficient CPU and Memory
      - PodMatchNodeSelector: if the Pod NodeSelector match the Node labels
      - CheckNodeDiskPressure: if a node is reporting a filesystem that is almost full, so that Pod won't be scheduling there
      - CheckVolumeBinding: if the volumes that are requested can be serviced by the node using bound and unbound PVCs.
    - In `Scoring`, the scheduler ranks all nodes remaining after filtering to choose the most suitable Pod placement. 
      - SelectorSpreadPriority: spreads Pods accross hosts, considering Pods that belong to the same Service, StatefulSet or Replicaset
      - LeastRequestedPriority: prioritizes nodes with fewer requested resources. 
      - NodeAffinityPriority: prioritizes nodes according to node affinity scheduling preferences. 

- **Node Affinity**
  - Node affinity is like nodeSelector and allows you to constrain which nodes a Pod is eligible to be scheduled on. 
  - Types of Node Affinity: 
    - **requiredDuringSchedulingIgnoredDuringExecution** : is a **hard requirement** and rules must be met for a Pod to be scheduled on a node
    - **prefferedDuringSchedulingIgnoredDuringExecution** : is a **soft requirement** and rules may not enforce a Pod to be scheduled on
      - a weight is used to indicate how hard the rules should be enforced. 

- **Pod Affinity**
  - Pod Affinity compares labels in Pods that need to be scheduled with labels of Pods already running on a node 
  - Pod Affinity negatively impacts performance in large clusters
  - Using Pod Affinity requires all nodes in the cluster to be labeled with a **topologyKey**

- **Managing Taints and Tolerations**
  - **Taints** are **applied to a node** to mark that the node should not accept any Pod that doesn't tolerate the taint
    - NoSchedule: doesn't schedule new Pods
    - PreferNoSchedule: doesn't schedule new Pods, unless there is no other option
    - NoExecute: migrates all Pods away from this node
    - **If the Pod has toleration however, it will ignore the taint**
  - **Tolerations** are **applied to a pod** and allow Pods to schedule on nodes with matching Taints 

</details>

## [10. Logging/Monitoring](https://kubernetes.io/docs/concepts/cluster-administration/logging/) 5%
<details> <summary> Details </summary>

* [Understand how to monitor all cluster components](https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/)
* Understand how to monitor applications
* [Manage cluster component logs](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/#looking-at-logs)
  * Master
    * /var/log/kube-apiserver.log - API Server, responsible for serving the API
    * /var/log/kube-scheduler.log - Scheduler, responsible for making scheduling decisions
    * /var/log/kube-controller-manager.log - Controller that manages replication controllers
  * Worker Nodes
    * /var/log/kubelet.log - Kubelet, responsible for running containers on the node
    * /var/log/kube-proxy.log - Kube Proxy, responsible for service load balancing
* [Manage application logs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs)
* Other: 
    * [Pod and Node metrics](https://kubernetes.io/docs/reference/kubectl/cheatsheet/#interacting-with-running-pods)
    * [Monitoring Kubernetes](https://www.datadoghq.com/blog/monitoring-kubernetes-era/)

</details>

***

## Practice Exam
* [CKA Practice Exam Environment](https://github.com/arush-sal/cka-practice-environment)

## Train Yourself Now (Exam Practices)
#### Using **curl** to Explore the API
- Use `curl` to explore which Pods are present in the kube-system namespace
<details><summary>Answer</summary>

```bash
kubectl proxy --port 8080 & 
curl http://localhost:8080/api/v1/namespaces/kube-system/pods
```

</details>

#### Managing Deployments
- Run a deployment that starts one nginx web server Pod on all cluster nodes
<details><summary>Answer</summary>

##### This question is related to Daemonsets. Go to the documentation and find sample yaml for Daemonset!

Sample yaml for Daemonset
```bash
$ vi nginx-daemonset.yaml
```
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    k8s-app: nginx-lable
spec:
  selector:
    matchLabels:
      name: nginx-ds
  template:
    metadata:
      labels:
        name: nginx-ds
    spec:
      containers:
      - name: nginx-ds
        image: nginx
```

```bash
$ kubectl create -f nginx-daemonset.yaml
    daemonset.apps/nginx-ds created

$ kubectl get pod -o wide 
    NAME             READY   STATUS    RESTARTS   AGE   IP          NODE                     NOMINATED NODE   READINESS GATES
    nginx-ds-2bjhg   1/1     Running   0          28s   10.44.0.1   vb-worker1.example.com   <none>           <none>
```

- alternative answer: 
```bash
# List your nodes
$ kubectl get nodes --show-labels
    NAME                     STATUS   ROLES    AGE   VERSION   LABELS
    vb-master.example.com    Ready    master   8d    v1.18.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=vb-master.example.com,kubernetes.io/os=linux,node-role.kubernetes.io/master=
    vb-worker1.example.com   Ready    <none>   8d    v1.18.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=vb-worker1.example.com,kubernetes.io/os=linux

# Add specific label to nodes so that App will be deployed there
$ kubectl label nodes <node-name> node-app=nginx

# Create yaml manually for deployment
$ kubectl create deployment test-nginx --image nginx --dry-run -o yaml > test-nginx.yaml

# edit the yaml and set the nodeselector
$ vi test-nginx.yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      creationTimestamp: null
      labels:
        app: test-nginx
      name: test-nginx
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: test-nginx
      strategy: {}
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: test-nginx
        spec:
          containers:
          - image: nginx
            name: nginx
            resources: {}
          nodeSelector:           # --> add this
            node-app: nginx       # --> add this
    status: {}

# Create deployment with yaml
$ kubectl create -f test-nginx.yaml

# Make sure apps run on Worker nodes
$ kubectl get pod -owide


```

</details>

#### ConfiguringStorage
- Configure a 2 GiB persistent storage solution that uses a permanent directory on the host that runs the Pod. Configure a Deployment that runs the httpd web server and mounts the storage on /var/www.
<details><summary>Answer</summary>

- Create PV (pv.yaml)
```bash
$ kubectl create -f pv.yaml
```

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: "/mnt/data"
```

- Create PVC (pvc.yaml)
```bash
$ kubectl create -f pvc.yaml
```
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi
```
- Create deployment yaml by dry-run (lab-httpd.yaml)
```bash
$ kubectl create deployment lab-httpd --image=httpd --dry-run -o yaml > lab-httpd.yaml
```

- Edit the deployment yaml as follow
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: lab-httpd
  name: lab-httpd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: lab-httpd
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: lab-httpd
    spec:
      volumes:
      - name: task-pv-storage
        persistentVolumeClaim:
          claimName: task-pv-claim
      containers:
      - image: httpd
        name: httpd
        volumeMounts:
        - name: task-pv-storage
          mountPath: "/var/www"
        resources: {}
status: {}
```
```bash
$ kubectl create -f lab-httpd.yaml
$ kubectl get pod,pv,pvc
```

</details>

#### Managing Pod Networking
- Create two services: myservice should be exposing port 9376 and forward to targetport 80, and mydb should be exposing port 9377 and forward to port 80. 
- Create a Pod that will start a busybox container that will sleep for 3600 seconds, but only if the aforesaid services are available. 
- To test that it is working, start the init container Pod before starting the services. 

<details><summary>Answer</summary>

- Create service by defining the yaml (go to check the Documentation)
  - service.yaml
```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9377
```
- Create the Pod with init container
```yaml
apiVersion: v1
kind: Pod
metadata: 
  name: init-pod
  labels:
    app: initapp
spec: 
  containers:
  - name: main-container
    image: busybox
    command: ['sh', '-c', 'echo main app running && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done']
  - name: init-db
    image: busybox
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done']

```


- Pod with init-container
```bash

```

</details>

#### ManagingScheduling
- Configure worker1.example.com in such a way that no new Pods will be scheduled on it, but existing Pods will not be moved away from it. 
- Mark worker1.example.com with the node label disk=ssd. Start a new nginx Pod that uses nodeAffinity to be scheduled on nodes that have the label disk=ssd set. Start the Pod and see what happens
- Remove all restrictions from worker1.example.com
<details><summary>Answer</summary>

**Answer1**

- Cordon worker1 so that no new Pods will be scheduled on it
```bash
# Cordon
kubectl cordon worker1.example.com
    node/worker1.example.com cordoned

# Check after the cordon. Make sure worker1 is now `SchedulingDisabled`
kubectl get nodes
    NAME                     STATUS                  ROLES    AGE   VERSION
    master.example.com    Ready                      master   18d   v1.18.2
    worker1.example.com   Ready,SchedulingDisabled   <none>   18d   v1.18.2

# Cordon doesn't effect the current running Pods
kubectl get pods -owide
    NAME             READY   STATUS    RESTARTS   AGE   IP          NODE                     NOMINATED NODE   READINESS GATES
    nginx-ds-2bjhg   1/1     Running   0          10d   10.44.0.1   worker1.example.com      <none>           <none>
```

- Mark worker1 with label disk=ssd
```bash
# Label node
kubectl label nodes worker1.example.com disk=ssd
    node/worker1.example.com labeled

# Check the current labels
kubectl get nodes --show-labels
    NAME                  STATUS                     ROLES    AGE   VERSION   LABELS
    master.example.com    Ready                      master   18d   v1.18.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=vb-master.example.com,kubernetes.io/os=linux,node-role.kubernetes.io/master=
    worker1.example.com   Ready,SchedulingDisabled   <none>   18d   v1.18.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=vb-worker1.example.com,kubernetes.io/os=linux,disk=ssd
```

- Start a Pod with nodeSelector disk=ssd. (Create yaml and run the pod)
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: scheduled-nginx
spec:
  containers:
  - name: sched-nginx
    image: nginx
  nodeSelector:
    disk: ssd
```

```bash
# Run the pod
kubectl create -f scheduled-nginx.yaml

# Check the pod
kuebctl get pod -o wide
    NAME              READY   STATUS    RESTARTS   AGE   IP          NODE                     NOMINATED NODE   READINESS GATES
    scheduled-nginx   0/1     Pending   0          7s    <none>      <none>                   <none>           <none>

# See what happened behind the `Pending` status
kubectl get events
    LAST SEEN   TYPE      REASON             OBJECT                MESSAGE
    <unknown>   Warning   FailedScheduling   pod/scheduled-nginx   0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) were unschedulable.
    <unknown>   Warning   FailedScheduling   pod/scheduled-nginx   0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) were unschedulable.
```

- Notice that the pod is Pending, since the selected Node (worker1) is currently cordoned `SchedulingDisabled`
- After that, remove the cordon. Notice that the pod will be normally scheduled to its node
```bash
# Uncordon worker1
kubectl uncordon worker1.example.com
    node/worker1.example.com uncordoned

# Check the pod status
kubectl get pod -o wide
    NAME              READY   STATUS    RESTARTS   AGE     IP          NODE                     NOMINATED NODE   READINESS GATES
    scheduled-nginx   1/1     Running   0          4m17s   10.44.0.2   worker1.example.com      <none>           <none>
```


kubectl taint nodes worker1.example.com key=value:NoSchedule
kubectl label nodes worker1.example.com disk=ssd
kubectl get nodes --show-labels

```bash

```

</details>

#### TemplateQuestionTopic
- Question Details
<details><summary>Answer</summary>

```bash

```

</details>


## Tips:

Get familiar with:
* [kubectl explain](https://blog.heptio.com/kubectl-explain-heptioprotip-ee883992a243)
* [kubectl cheatsheet](https://kubernetes.io/docs/user-guide/kubectl-cheatsheet/)
* How to switch namespaces in Kubernetes?
```bash
kubectl config set-context --current --namespace=<namespace>
```
```bash
$ kubectl config current-contexts # check your current context 
$ kubectl config set-context <context-of-question> --namespace=<namespace-of-question>
$ kubectl config current-context 
$ kubectl config view 
$ kubectl config get-contexts
$ kubectl config view | grep namespace
```
* When using kubectl for investigations and troubleshooting utilize the wide output it gives your more details
```
     $kubectl get pods -o wide --show-labels --all-namespaces
```
* In `kubectl` utilizie `--all-namespaces` to ensure deployments, pods, objects are on the right name space, and right desired state

* for events and troubleshooting utilize kubectl describe
```
     $kubectl describe pods <PODID>
```
* the '-o yaml' in conjuction with `--dry-run` allows you to create a manifest template from an imperative spec, combined with `--edit` it allows you to modify the object before creation
```
kubectl create service clusterip my-svc -o yaml --dry-run > /tmp/srv.yaml
kubectl create --edit -f /tmp/srv.yaml
```
## Do you want more?

- [Kubernauts resources list](https://docs.google.com/spreadsheets/d/10NltoF_6y3mBwUzQ4bcQLQfCE1BWSgUDcJXy-Qp2JEU/edit#gid=0)

#### Lab environment
```bash
#terapod
192.168.11.14	master.example.com
192.168.11.15	worker1.example.com
192.168.11.16	worker2.example.com


192 /et.168.11.5    putihfj
192.168.11.19   putihfj-back
192.168.11.25   vb-master.example.com
192.168.11.26   vb-worker1.example.com


#Command to turn on virtual machine by command
/usr/lib/vmware/bin/vmplayer /root/vmware/kb-worker1/kb-worker1.vmx
https://docs.vmware.com/en/VMware-Workstation-Player-for-Linux/15.0/com.vmware.player.linux.using.doc/GUID-BF62D91D-0647-45EA-9448-83D14DC28A1C.html

#But, I would recommend you Virtual Box ^^;

```
